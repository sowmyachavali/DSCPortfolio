---
title: "pizza restaurants analysis"
author: "Sowmya Chavali"
date: "02/04/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
always_allow_html: yes
---

Pizza! It’s one of America’s most beloved dishes, eaten anywhere, anytime, and by everyone. The objective of this project is to find excellent pizza based on the location, review and price, and finally build a pizza-map focused on New York, which is known as the “Pizza Capital of the U.S.”

I am using the data sources from Jared, Barstool, and Datafiniti. Jared’s data is from top NY pizza restaurants, with a 6-point Likert scale survey on ratings. The Barstool sports dataset has critic, public, and the Barstool Staff’s rating as well as pricing, location, and geo-location. Datafiniti includes 10000 pizza places, their price ranges and geo-locations.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results='hide', message=FALSE,warning=FALSE}
library(readr)
library(dplyr)
library(tidyverse)
library(ggplot2) 
library(ggmap)
library(car)
library(plotly)
library(bootstrap)
library(cluster)
library(factoextra)
library(gridExtra)
library(leaflet)
library(DataExplorer)
```

##### Importing Data

```{r results='hide',message=FALSE,warning=FALSE}
barstool <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv")
datafiniti <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv")
jared <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv")
```

The purpose of the data is to find the best pizza restaurant, focusing New York City. To achieve that Tyler Ricards recorded the web traffic coming through the OneBite application.

##### Source data breakdown

```{r}
dim(barstool)
dim(datafiniti)
dim(jared)
```

Identifying missing values
```{r}
colSums(is.na(barstool))  # 2 missing observations - latitude and longitude
unique_datafiniti <- datafiniti %>% distinct() 
colSums(is.na(unique_datafiniti)) # No missing observations
colSums(is.na(jared))   # 5 missing observations - percent
```

No of Zero values in each column
```{r}
colSums(barstool == 0)
colSums(unique_datafiniti == 0)
colSums(jared == 0)
```

Summary of each dataset
```{r}
summary(barstool)
summary(datafiniti)
summary(jared)

table(barstool$price_level)
```
0 and 3 price level have few observations

##### Data Cleaning
Generally we employ data cleaning steps to derive relevant insights from the data and to get rid of garbage values
There are only two missing values in the latitude and longitude column in the Barstool data. Also, there is a value out of range in Dave’s rating.No imputation is being performed as the count is less than 5% of the data in both cases.Critic ratings have 401 zeroes, Community ratings have 41 zeroes, Minimum price range contains 1852 zeroes, Votes have 104 zeroes. Apart from this the data is clean.

Cleaning Datafiniti

Lets create Category column by clubbing similar categories

```{r}
head(unique_datafiniti)
```

```{r}
unique_datafiniti$categories <- toupper(unique_datafiniti$categories)
new_datafiniti <- unique_datafiniti %>% 
mutate(category = case_when(str_detect(categories,"BAR|BREW|PUB|CLUB|LOUNGE") ~ 'ALCOHOL SERVING', str_detect(categories,"ITAL") ~ 'ITALIAN',str_detect(categories,"CATER") ~ 'CATERERS', TRUE ~ 'NORMAL PIZZA RESTAURANT'))
```

Checking the category column
```{r}
table(new_datafiniti$category)
```

Lets clean jared data
```{r}
dim(jared)
# Removing rows with 0 total votes
jared_rmzero <- jared%>%
               filter(total_votes != 0)
#Checking new data

dim(jared_rmzero)

DT::datatable(barstool)
DT::datatable(unique_datafiniti)
DT::datatable(jared_rmzero)
```

Converting answer to Numerical Rating

```{r}
jared_rmzero <- jared_rmzero %>%
  mutate(Numerical_Rating = case_when(
    answer=="Never Again" ~ 0,
    answer=="Poor" ~ 2,
    answer=="Fair" ~ 4,
    answer=="Average" ~ 6,
    answer=="Good"~ 8,
    answer=="Excellent" ~ 10))

# Calculating weighted numerical rating
jared_ratings <- jared_rmzero %>% 
  mutate(Weighted_Rating = Numerical_Rating*votes) %>%
    group_by(place) %>%
    summarise(Final_Rating = sum(Weighted_Rating)/sum(votes))

# Looking at the final Jared Ratings
head(jared_ratings)
```


##### EDA

I have applied DataExplorer package as a fast and efficient way to do typical basic EDA. Through the Plot Bar chart of Datafiniti, we found that New York is the dominant state (province) in the dataset.

```{r}
plot_bar(datafiniti$province)
```

Correlation between various pizza ratings
```{r}
barstool_2 <- barstool %>% 
  rename(
    all_score = review_stats_all_average_score,
    community_score = review_stats_community_average_score,
    critic_score = review_stats_critic_average_score,
    dave_score = review_stats_dave_average_score
    )

data <- barstool_2 %>% select(provider_rating,community_score,critic_score,dave_score)
data2 <- data[data$critic_score != 0 & data$community_score != 0,]
cor(data2)
```
Correlation between critic score and dave score is 0.42

```{r}
data3 <- data[data$community_score != 0,]
cor(data3)
```
Correlation between dave score and community score is 0.6, between provider_rating and community score is 0.32 and correlation between provider_rating and dave score is 0.22

Joining Barstool and jared

```{r}
barstool_jared<- jared_ratings %>% 
  inner_join(barstool, by = c("place" = "name"))
```
Finding correlation between Jared Final Rating and Barstool All Average Rating

```{r}
cor(barstool_jared$Final_Rating,barstool_jared$review_stats_all_average_score) ## The correlation is not very high
```

##### Comparing pizza ratings in New York with rest of the US
```{r}
Newyork_Barstool <- barstool[str_detect(barstool$city,"York"),]
Rest_of_US_Barstool <-barstool[!str_detect(barstool$city,"York"),]
mean(Newyork_Barstool$review_stats_all_average_score)
mean(Rest_of_US_Barstool$review_stats_all_average_score)
mean(Newyork_Barstool$provider_rating)
mean(Rest_of_US_Barstool$provider_rating)
```

New York has slightly lower provider and average ratings on average as compared to the rest of US

##### Comparing pizza ratings across states
```{r}
table((barstool %>% left_join(new_datafiniti%>% distinct(city,province),by = "city"))$province)     ## States except NY have few records

table1 <- barstool %>% left_join(new_datafiniti%>% distinct(city,province),by = "city") %>% group_by(province) %>% summarise(Avg_provider_rating = mean(provider_rating)) %>% arrange(desc(Avg_provider_rating))   

table1 = na.omit(table1)

## Plotting state wise average provider ratings

ggplot(data = table1, aes(x = reorder(province, -Avg_provider_rating), y = Avg_provider_rating)) +
  geom_bar(stat = "identity") +                        
    xlab("state") + ylab("Average Provider Rating") + 
    ggtitle("Statewise Provider rating") +     
    theme_bw() + geom_text(aes(label=round(Avg_provider_rating,2)), position=position_dodge(width=0.9), vjust=-0.25)+
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
table2 <- barstool %>% left_join(new_datafiniti%>% distinct(city,province),by = "city") %>% group_by(province) %>% summarise(Avg_All_Rating = mean(review_stats_all_average_score)) %>% arrange(desc(Avg_All_Rating))

table2 = na.omit(table2)

## Plotting state wise All average ratings

ggplot(data = table2, aes(x = reorder(province, -Avg_All_Rating), y = Avg_All_Rating)) +
  geom_bar(stat = "identity",
             size=.2) +                        
    xlab("state") + ylab("average all rating") + 
    ggtitle("statewise avg all average rating") +     
    theme_bw() + geom_text(aes(label=round(Avg_All_Rating,2)), position=position_dodge(width=0.9), vjust=-0.25)+
  theme(plot.title = element_text(hjust = 0.5))
```

##### Comparing ratings across categories

Joining Datafinity and Barstool data

``` {r}
datafiniti_barstool<- new_datafiniti %>% 
  inner_join(barstool, by = "name", "city")

dim(datafiniti_barstool)
```

Analysing ratings across pizza categories

``` {r}
boxplot(review_stats_all_average_score~category, data = datafiniti_barstool)
boxplot(provider_rating~category, data = datafiniti_barstool)
```

Normal Pizza Restaurants have slightly higher All average score as compared to those restaurants which serve Italian pizza, however provider_rating has very similar distribution across both the categories

##### Comparing price range across pizza categories
```{r}
new_datafiniti %>% group_by(category) %>% summarise(AVERAGE_MAX_PRICE = mean(price_range_max))
new_datafiniti[new_datafiniti$price_range_min != 0,] %>% group_by(category) %>% summarise(AVERAGE_MIN_PRICE = mean(price_range_min))
```

Alcohol serving pizza restaurants have the highest average min and max price range followed by Italian pizza restaurants. Caterers and Normal Pizza restaurants have similar min and max price range.



###### Do higher priced restaurants have better ratings?
Lets analyze provider ratings
```{r}
price_low <- barstool[(barstool$price_level  == 1) | (barstool$price_level  == 0),]
price_high <- barstool[(barstool$price_level  == 2) | (barstool$price_level  == 3),]

mean_high <- mean(price_high$provider_rating)
mean_high
mean_low <- mean(price_low$provider_rating)
mean_low

nrow_high <- nrow(price_high)
nrow_high

nrow_low <- nrow(price_low)
nrow_low
```

NULL HYPOTHESIS: mean_high - mean_low <= 0 
ALTERNATE HYPOTHESIS mean_high - mean_low > 0
```{r}
se = sqrt(var(price_high$provider_rating)/nrow_high + var(price_low$provider_rating)/nrow_low)
se

Z = (mean_high-mean_low)/se
Z

Zalpha = qnorm(0.90)
Zalpha
```

Z > Zalpha
We can reject the NULL HYPOTHESIS with 90% confidence. Hence we can say higher priced restaurants have better mean provider_ratings as compared to lower priced restaurants with 90% confidence

Analyzing All Average Score

```{r}
u1 <- mean(price_high$review_stats_all_average_score)
u1

u2 <- mean(price_low$review_stats_all_average_score)
u2
```

NULL HYPOTHESIS: u1 - u2 <= 0 ALTERNATE HYPOTHESIS u1 - u2 > 0

```{r}
se2 = sqrt(var(price_high$review_stats_all_average_score)/nrow_high + var(price_low$review_stats_all_average_score)/nrow_low)
se2

Z2 = (u1-u2)/se2
Z2

Zalpha2 = qnorm(0.99)
Zalpha2
```

Z2 > Zalpha2

We can reject the NULL HYPOTHESIS with 99% confidence. Hence we can say higher priced restaurants have better mean all_average_score as compared to lower priced restaurants with 99% confidence.


###### Regression for predicting community ratings - OneBite user ratings

```{r}

# BOXCOX TRANSFORMATION TO GET LAMBDA
boxcox <- MASS::boxcox(community_score ~ dave_score + provider_rating, data = data3)

lambda <- boxcox$x[which.max(boxcox$y)]
lambda

data3$community_score2 <- ((data3$community_score ^ lambda) - 1) / lambda

## POLYNOMIAL REGRESSION FOR PREDICTING COMMUNITY RATINGS

fit <- lm(community_score2 ~ poly(dave_score,2) + poly(provider_rating,5), data = data3)

```

Adjusted R square of the model is 0.46. Since we are predicting consumer ratings which has very high variation, we can accept this R square value. Also, p value associated with F test and most of the individual t tests in not significant and we can reject the NULL hypothesis at 95% confidence level.

```{r}
summary(fit)
```

Regression equation


$$community\_score2 = \frac{community\_score^\lambda-1}{\lambda}$$

$$community\_score2 = 25.3705 + 93.4312*dave\_score + 33.6138*dave\_score^2 + 31.7952*provider\_rating + 7.3717*provider\_rating^2 -9.4294*provider\_rating^3 + 12.9945*provider\_rating^4 - 13.2177*provider\_rating^5$$


Plotting and analyzing the residuals

By performing residual diagnostic, we can see that they satisfy our initial regression assumptions of-

* Normality
* Constant Variance
* Mean 0

```{r}
plot(fit)
```

###### Clustering

We are going to cluster pizza restaurants on the basis of price level and community ratings

```{r}
#Removing zeroes
barstool_rm <- barstool[barstool$review_stats_community_average_score!=0,]
#Scaling data 
barstool_cl <- scale(barstool_rm[c("review_stats_community_average_score","price_level")])

## Creating multiple clusters with different centres

set.seed(5021)

k2 <- kmeans(barstool_cl, centers = 2, nstart = 25)
k3 <- kmeans(barstool_cl, centers = 3, nstart = 25)
k4 <- kmeans(barstool_cl, centers = 4, nstart = 25)
k5 <- kmeans(barstool_cl, centers = 5, nstart = 25)

str(k4)
# Visualizing the clusters

p2 <- fviz_cluster(k2, geom = "point", data = barstool_cl) + ggtitle("k = 2")
p3 <- fviz_cluster(k3, geom = "point", data = barstool_cl) + ggtitle("k = 3")
p4 <- fviz_cluster(k4, geom = "point", data = barstool_cl) + ggtitle("k = 4")
p5 <- fviz_cluster(k5, geom = "point", data = barstool_cl) + ggtitle("k = 5")

grid.arrange(p2, p3, p4, p5, nrow = 2)
#Elbow Curve to decide the optimum number of clusters looking at the bend

fviz_nbclust(barstool_cl, kmeans, method = "wss")


# Comparing the clusters

barstool_rm %>%
  select("review_stats_community_average_score","price_level") %>%
  mutate(Cluster = k4$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

So finally we have 4 cluster which signify

* Cluster_1 - Low Rating and Low Price restaurants
* Cluster_2 - High Rating and High Price restaurants
* Cluster_3 - Low Rating and High Price restaurants
* Cluster_4 - High Rating and Low Price restaurants

##### Visualization

```{r}
#Based on Ratings

barstool_NY <- barstool[barstool$city=='New York',]%>%
  na.omit(barstool_NY)
  
getColor <- function(barstool_NY) {
    sapply(barstool_NY$review_stats_all_average_score, function(x) {
      if(x <= 4.5) {
        "red"
      } else if(x <= 6.5) {
        "orange"
      } else {
        "green"
      } })
  }
  
  icons <- awesomeIcons(
    icon = 'ios-close',
    iconColor = 'black',
    library = 'ion',
    markerColor = getColor(barstool_NY)
  )
  
  
  leaflet(barstool_NY) %>% 
    addTiles() %>%
    addAwesomeMarkers(~longitude, ~latitude, icon=icons, label=~as.character(name))%>%
    addProviderTiles("CartoDB.Positron") %>%
    setView(-73.98, 40.75, zoom = 14)
  
  
  #Based on Clusters
  
  clustered_data <- cbind(k4$cluster,barstool_rm)%>%
    na.omit(barstool_rm$latitude) %>%
    na.omit(barstool_rm$longitude) %>%
    filter(city=="New York")

clustered_data['cluster'] <- clustered_data['k4$cluster'] 

dim(clustered_data)






getColor <- function(clustered_data) {
    sapply(clustered_data$cluster, function(x) {
      if(x == 1) {
        "pink"
      } else if(x == 2) {
        "green"
      } else if(x == 3) {
        "orange"
      }else {
        "red"
      } })
  }
  
  icons <- awesomeIcons(
    icon = 'ios-close',
    iconColor = 'black',
    library = 'ion',
    markerColor = getColor(clustered_data)
  )
  
  
  leaflet(clustered_data) %>% 
    addTiles() %>%
    addAwesomeMarkers(~longitude, ~latitude, icon=icons, label=~as.character(name))%>%
    addProviderTiles("CartoDB.Positron") %>%
    setView(-73.98, 40.75, zoom = 12.5)
  ```
  
Cluster_1 PINK - Low Rating and Low Price restaurants
Cluster_2 GREEN - High Rating and High Price restaurants
Cluster_3 ORANGE - Low Rating and High Price restaurants
Cluster_4 RED - High Rating and Low Price restaurants



The above exercise helped us understand various trends in pizza ratings. The following is the summary of the analysis:

We find that there is low correlation between commmunity, provider, critic and jared ratings. Community and Dave ratings have moderately high correlation of ~0.6
New York have lower provider and average pizza ratings on average as compared to rest of the US
States with high ratings- IA,OK,FL,OH States with low ratings - WV,MI,NV,SC
Restaurants serving Italian Pizza have lower ratings on average as compared to Non Italian Pizza restaurants
Alcohol serving and Italian pizza restaurants have higher priced pizza as compared to those that do not fall in this category
High priced restaurants have better pizza ratings as compared to low priced restaurants
